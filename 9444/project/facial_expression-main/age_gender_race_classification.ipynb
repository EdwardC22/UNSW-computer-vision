{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age, Gender and Race Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install poutyne\n",
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "from torch.utils import model_zoo\n",
    "from torch.utils.data import Subset, DataLoader, Dataset, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from poutyne import set_seeds, Model, ModelCheckpoint, CSVLogger, Experiment, BatchMetric\n",
    "import tarfile\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './datasets/utk_faces'\n",
    "tar_file_name = 'utk_face.tar.gz'\n",
    "base_image_path = os.path.join(base_path, 'images')\n",
    "download_url = 'https://drive.google.com/uc?export=download&id=0BxYys69jI14kYVM3aVhKS1VhRUk&alt=media&confirm=t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset to ./datasets/utk_faces\\utk_face.tar.gz ...\n",
      "Extracting archive...\n"
     ]
    }
   ],
   "source": [
    "def download_and_extract_dataset(base_path, extract_path, file_name, url):\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    os.makedirs(extract_path, exist_ok=True)\n",
    "    tar_filename = os.path.join(base_path, file_name)\n",
    "\n",
    "    print(f'Downloading dataset to {tar_filename} ...')\n",
    "    urllib.request.urlretrieve(url, tar_filename)\n",
    "    print('Extracting archive...')\n",
    "    with tarfile.open(tar_filename, \"r\") as tar:\n",
    "        tar.extractall(extract_path)\n",
    "    \n",
    "download_and_extract_dataset(base_path, base_image_path, tar_file_name, download_url)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Training constants\n",
    "cuda_device = 0\n",
    "device = torch.device(\"cuda:%d\" % cuda_device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_classes = 7\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 0.01\n",
    "n_epoch = 70\n",
    "image_size = 48\n",
    "loss_weights={'age': 1/24, 'gender': 1/2, 'race': 1/5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacesImageFolder(ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (sample, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        filename = os.path.basename(path)\n",
    "        age, gender, race, _ = filename.split('_')\n",
    "            \n",
    "        target = [int(int(age) / 5), int(gender), int(race)]\n",
    "\n",
    "        return sample, target\n",
    "    \n",
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item, label = self.dataset[index]\n",
    "        if self.transform is None:\n",
    "              return item, label\n",
    "        \n",
    "        return self.transform(item), label  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_transform = transforms.Compose(\n",
    "    [                \n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.RandomAffine(degrees = 0, translate = (0.1, 0.1)),\n",
    "        #transforms.RandomAutocontrast(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip(),\n",
    "        #transforms.RandomPerspective(),    \n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "normalize_transform = transforms.Compose(\n",
    "    [        \n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "re_valid_file_name = re.compile('\\d+_\\d_\\d_.*')\n",
    "images_dataset = FacesImageFolder(\n",
    "    base_image_path,\n",
    "    transform=transforms.ToTensor(),\n",
    "    is_valid_file=lambda path: not os.path.split(path)[1].startswith('.') and re_valid_file_name.match(os.path.basename(path)),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(images_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_lengths = [int(len(images_dataset) * 0.8), int(len(images_dataset) * 0.1), 0]\n",
    "dataset_lengths[2] = len(images_dataset) - dataset_lengths[0] - dataset_lengths[1]\n",
    "train_dataset, valid_dataset, test_dataset = random_split(images_dataset, dataset_lengths)\n",
    "\n",
    "train_dataset = TransformDataset(train_dataset, augment_transform)\n",
    "valid_dataset = TransformDataset(valid_dataset, normalize_transform)\n",
    "test_dataset = TransformDataset(test_dataset, normalize_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(valid_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the PyTorch's dataloader to split our data into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dataset):\n",
    "    fig, ax = plt.subplots(4, 4, figsize=(12, 12))\n",
    "    for i, (image, _) in enumerate(dataset):\n",
    "        ax[i % 4, i // 4].imshow(image.permute(1, 2, 0))\n",
    "        if i >= 15:\n",
    "            break\n",
    "            \n",
    "show_batch(images_dataset)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(train_loader))   \n",
    "print(inputs.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding='same')    \n",
    "        self.conv1_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding='same')\n",
    "        self.bn_conv1 = nn.BatchNorm2d(64)\n",
    "        self.dropout_conv1 = nn.Dropout2d(0.25)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding='same')\n",
    "        self.bn_conv2 = nn.BatchNorm2d(128)\n",
    "        self.dropout_conv2 = nn.Dropout2d(0.25)        \n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=512, kernel_size=3, padding='same')\n",
    "        self.bn_conv3 = nn.BatchNorm2d(512)\n",
    "        self.dropout_conv3 = nn.Dropout2d(0.25)        \n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding='same')\n",
    "        self.bn_conv4 = nn.BatchNorm2d(512)    \n",
    "        self.dropout_conv4 = nn.Dropout2d(0.25) \n",
    "        \n",
    "        #flatten_length =  c * image_size * image_size        \n",
    "        flatten_length = 4608\n",
    "        self.fc1_age = nn.Linear(flatten_length, 256)\n",
    "        self.bn_fc1_age = nn.BatchNorm1d(256)\n",
    "        self.dropout_fc1_age = nn.Dropout(0.25)\n",
    "        \n",
    "        self.fc1_gender = nn.Linear(flatten_length, 256)\n",
    "        self.bn_fc1_gender = nn.BatchNorm1d(256)\n",
    "        self.dropout_fc1_gender = nn.Dropout(0.25)\n",
    "        \n",
    "        self.fc1_race = nn.Linear(flatten_length, 256)\n",
    "        self.bn_fc1_race = nn.BatchNorm1d(256)\n",
    "        self.dropout_fc1_race = nn.Dropout(0.25)\n",
    "        \n",
    "        self.fc2_age = nn.Linear(256, 512)\n",
    "        self.bn_fc2_age = nn.BatchNorm1d(512)\n",
    "        self.dropout_fc2_age = nn.Dropout(0.25)    \n",
    "        \n",
    "        self.fc2_gender = nn.Linear(256, 512)\n",
    "        self.bn_fc2_gender = nn.BatchNorm1d(512)\n",
    "        self.dropout_fc2_gender = nn.Dropout(0.25)\n",
    "        \n",
    "        self.fc2_race = nn.Linear(256, 512)\n",
    "        self.bn_fc2_race = nn.BatchNorm1d(512)\n",
    "        self.dropout_fc2_race = nn.Dropout(0.25)\n",
    "        \n",
    "        self.fc3_out_age = nn.Linear(512, 24)\n",
    "        self.fc3_out_gender = nn.Linear(512, 2)\n",
    "        self.fc3_out_race = nn.Linear(512, 5)\n",
    "        \n",
    "    def forward(self, input):    \n",
    "        output = F.relu(self.conv1_1(input))      \n",
    "        output = F.relu(self.conv1_2(output))\n",
    "        output = self.bn_conv1(output)    \n",
    "        output = self.maxpool(output)   \n",
    "        output = self.dropout_conv1(output)\n",
    "        \n",
    "        output = F.relu(self.conv2(output))\n",
    "        output = self.bn_conv2(output)    \n",
    "        output = self.maxpool(output)   \n",
    "        output = self.dropout_conv2(output)\n",
    "        \n",
    "        output = F.relu(self.conv3(output))\n",
    "        output = self.bn_conv3(output)        \n",
    "        output = self.maxpool(output)    \n",
    "        output = self.dropout_conv3(output)\n",
    "        \n",
    "        output = F.relu(self.conv4(output))\n",
    "        output = self.bn_conv4(output)\n",
    "        output = self.maxpool(output)            \n",
    "        output = self.dropout_conv4(output)    \n",
    "        \n",
    "        # Flattening process\n",
    "        b, c, h, w = output.size() # batch_size, channels, height, width\n",
    "        output = output.view(-1, c * h * w)\n",
    "        \n",
    "        output_age = self.fc1_age(output)\n",
    "        output_age = self.bn_fc1_age(output_age)\n",
    "        output_age = self.dropout_fc1_age(output_age)\n",
    "        \n",
    "        output_gender = self.fc1_gender(output)\n",
    "        output_gender = self.bn_fc1_gender(output_gender)\n",
    "        output_gender = self.dropout_fc1_gender(output_gender)   \n",
    "        \n",
    "        output_race = self.fc1_race(output)\n",
    "        output_race = self.bn_fc1_race(output_race)\n",
    "        output_reace = self.dropout_fc1_race(output_race)   \n",
    "        \n",
    "        output_age = self.fc2_age(output_age)\n",
    "        output_age = self.bn_fc2_age(output_age)\n",
    "        output_age = self.dropout_fc2_age(output_age)        \n",
    "        output_age = self.fc3_out_age(output_age)\n",
    "        \n",
    "        output_gender = self.fc2_gender(output_gender)\n",
    "        output_gender = self.bn_fc2_gender(output_gender)\n",
    "        output_gender = self.dropout_fc2_gender(output_gender)        \n",
    "        output_gender = self.fc3_out_gender(output_gender)\n",
    "        \n",
    "        output_race = self.fc2_race(output_race)\n",
    "        output_race = self.bn_fc2_race(output_race)\n",
    "        output_race = self.dropout_fc2_race(output_race)        \n",
    "        output_race = self.fc3_out_race(output_race)\n",
    "        \n",
    "        return output_age, output_gender, output_race\n",
    "    \n",
    "cnn = CNN().to(device)\n",
    "print(summary(cnn, input_size=(3, image_size, image_size)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We define callbacks for saving last epoch, best epoch and logging the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeGenderRaceLoss(nn.Module):\n",
    "    def __init__(self, weights=None):\n",
    "        super(AgeGenderRaceLoss, self).__init__()\n",
    "        if weights is None:\n",
    "            weights = {'age': 1/3, 'gender': 1/3, 'race': 1/3}\n",
    "            \n",
    "        self.weights = weights\n",
    " \n",
    "    def forward(self, inputs, targets, smooth=1):        \n",
    "        cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "                \n",
    "        [age_inputs, gender_inputs, race_inputs] = inputs\n",
    "        [age_targets, gender_targets, race_targets] = targets    \n",
    "\n",
    "        age_loss = cross_entropy_loss(age_inputs, age_targets)\n",
    "        gender_loss = cross_entropy_loss(gender_inputs, gender_targets)\n",
    "        race_loss = cross_entropy_loss(race_inputs, race_targets)    \n",
    "\n",
    "        weighted_loss = age_loss * self.weights['age'] +  gender_loss *  self.weights['gender'] + race_loss * self.weights['race']        \n",
    "        return age_loss\n",
    "    \n",
    "class AgeGenderRaceAccuracy(BatchMetric):\n",
    "    def __init__(self, *,  weights=None):\n",
    "        super().__init__()\n",
    "        self.__name__ = 'acc'\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        batch_size = y_pred[0].shape[0]\n",
    "        [y_true_age, y_true_gender, y_true_race] = y_true\n",
    "        [y_pred_age, y_pred_gender, y_pred_race] = y_pred\n",
    "        \n",
    "        y_pred_age = y_pred_age.argmax(1)\n",
    "        y_pred_gender = y_pred_gender.argmax(1)\n",
    "        y_pred_race = y_pred_race.argmax(1)\n",
    "        \n",
    "        acc_pre_age = (y_pred_age == y_true_age).float()\n",
    "        acc_pred_gender = (y_pred_gender == y_true_gender).float()\n",
    "        acc_pred_race = (y_pred_race == y_true_race).float()\n",
    "    \n",
    "\n",
    "        acc_pred = acc_pre_age.sum() + acc_pred_gender.sum() + acc_pred_race.sum()\n",
    "            \n",
    "        #return acc_pred * 100 / batch_size / 3\n",
    "        return acc_pred_race.sum() / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We are saving everything into ./saves/cub200.\n",
    "save_base_dir = 'saves'\n",
    "save_path = os.path.join(save_base_dir, 'cnn')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "callbacks = [\n",
    "    # Save the latest weights to be able to resume the optimization at the end for more epochs.\n",
    "    ModelCheckpoint(os.path.join(save_path, 'last_epoch.ckpt')),\n",
    "    # Save the weights in a new file when the current model is better than all previous models.\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(save_path, 'best_epoch_{epoch}.ckpt'),\n",
    "        monitor='val_acc',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        restore_best=True,\n",
    "        verbose=True,\n",
    "    ),\n",
    "    # Save the losses and accuracies for each epoch in a TSV.\n",
    "    CSVLogger(os.path.join(save_path, 'log.tsv'), separator='\\t'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_parameters = (parameter for name, parameter in cnn.named_parameters() if name in ['conv3.weight','conv4.weight'])\n",
    "non_weight_parameters = (parameter for name, parameter in cnn.named_parameters() if name not in ['conv3.weight','conv4.weight'])\n",
    "\n",
    "optimizer = optim.Adam([\n",
    "    {'params': weight_parameters, 'weight_decay': weight_decay},\n",
    "    {'params': non_weight_parameters}\n",
    "  ], lr=learning_rate)\n",
    "\n",
    "loss_function = AgeGenderRaceLoss(loss_weights)\n",
    "#loss_function = 'L1Loss'\n",
    "\n",
    "model = Model(\n",
    "    cnn,\n",
    "    optimizer,\n",
    "    loss_function,\n",
    "    batch_metrics=[AgeGenderRaceAccuracy()],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "model.fit_generator(\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    epochs=n_epoch,\n",
    "    callbacks=callbacks    \n",
    ")\n",
    "\n",
    "test_loss, test_acc, y_predict, y_true = model.evaluate_generator(test_loader, return_pred=True, return_ground_truth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logs = pd.read_csv(os.path.join(save_path, 'log.tsv'), sep='\\t')\n",
    "print(logs)\n",
    "\n",
    "best_epoch_idx = logs['val_acc'].idxmax()\n",
    "best_epoch = int(logs.loc[best_epoch_idx]['epoch'])\n",
    "print(\"Best epoch: %d\" % best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics = ['loss', 'val_loss']\n",
    "plt.plot(logs['epoch'], logs[metrics])\n",
    "plt.legend(metrics)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metrics = ['acc', 'val_acc']\n",
    "plt.plot(logs['epoch'], logs[metrics])\n",
    "plt.legend(metrics)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_true, np.argmax(y_predict, axis=1))\n",
    "class_names = list(train_valid_dataset.class_to_idx)\n",
    "\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, fmt='.2%', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.savefig('plots/confusion_matrix.png')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
